
Graph Clustering:

http://www.leonidzhukov.net/hse/2016/networks/papers/GraphClustering_Schaeffer07.pdf
https://www.csc2.ncsu.edu/faculty/nfsamato/practical-graph-mining-with-R/slides/pdf/Graph_Cluster_Analysis.pdf
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.30&rep=rep1&type=pdf


GraphX Tutorial by Berkeley:
http://ampcamp.berkeley.edu/6/exercises/graph-analytics-with-graphx.html


https://blogs.oracle.com/datascience/graph-computations-with-apache-spark
https://www.linkedin.com/pulse/connected-component-using-map-reduce-apache-spark-shirish-kumar/
http://note.yuhc.me/2015/03/graphx-property-operator-2/


Validation Strategy
The validation is a process of model performance evaluation.
Classic mistace is to use small data set for the model training or validate model on the same data set as train it.
There are a number of strategies to validate the model.
One of them is to split the available data into train and test data
another one is to perform a cross validation: process of splitting
the data on k beans equal size; run learning experiments; repeat this operation number of times and take the average test result.

https://towardsdatascience.com/deepgl-on-neo4j-b27e8c64190f

https://arxiv.org/pdf/1607.00653.pdf

https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fef

https://olegleyz.github.io/enron_classifier.html

Data Exploration
The features in the data fall into three major types, namely financial features, email features and POI labels.
There are 146 samples with 20 features and a binary classification ("poi"), 2774 data points.
Among 146 samples, there are 18 POI and 128 non-POI.
Among 2774, there are 1358 (48.96%) data points with NaN values.

http://bailando.sims.berkeley.edu/enron_email.html
bu bailando sims berkeley adresinde dataset, etiketler vs anlatılıyor

https://data.world/brianray/enron-email-dataset
son iki bağlantı daha anlamlı görünüyor, diğerleri ile vakit kaybetme doğrudan bunlara bakalım dilersen

https://www.sciencenews.org/article/information-flow-can-reveal-dirty-deeds

https://stanford.edu/~rezab/sparkclass/slides/ankur_graphx.pdf
yansı 10

https://rdrr.io/cran/igraphdata/man/enron.html

https://beta.vu.nl/nl/Images/werkstuk-eems_tcm235-414143.pdf

We created a network based on the senders and receivers in the Enron email dataset, whereas, the weight of the
edge between two individuals is the sum of the number of emails between them.
First, we use visualization techniques to analyze the graph. Second we use statistics, where we use the generated
 graph of visualization. With each technique, we try to use information gathered by previously. Main findings
  We used the techniques ranking and partitioning, which are visualization techniques. We found three possible
  fraudulent individuals (one is debatable) out of five individuals for the ranking technique. For the indication,
  whether, someone is possible fraudulent we used a web search. Furthermore, the two individuals who had a much higher
  ranking value than the others, were both indicated as possible fraudulent. However, for the technique partitioning,
  the three individuals that were selected were not possible fraudulent.
We generated the egocentric networks of the nodes with the highest ranking values and looked at their overlap.
We found one individual with a strong connection to both, after a web search we indicated her as possible fraudulent.
 Furthermore, we looked at hubs and authorities using statistics, of the nine individuals we found, we indicated four
  as possible fraudulent. Therefore, we conclude that it is possible to perform outlier detection on a network using
  graph mining to indicate possible fraudulent individuals in a communication network


Because we had only limited time (one month) to perform this research, our research has some limitations.
We selected the scope only to look at the network itself and not at the content of the emails. Therefore, we disregarded a substantial amount of information.

Furthermore, this paper is focussed on researching outlier detection using graph mining specifically on the Enron email dataset.
Therefore, we can only give an indication of the quality and effectiveness of the graph mining techniques on outlier detection.
At last, we have done a web search to indicate if individuals are fraudulent concerning Enron. However, these indications do not have to be true.

Furthermore, we found too little information for some of the individuals to make an indication.


Distributed K-Betweenness:
The number of shortest paths from all vertices to all others that pass through that node.
Independent computation for each node. Not implemented in GraphX.
Algorithm:
    Divide nodes between machines
    For each machine, compute the Betweenness contribution of each node to every other node in the graph
    Aggregate results from all machines
Problems:
    Can’t get information about a specific node in GraphX
    Need to copy graph to every machine (goes bad with big graphs)
Solutions:
    Can’t get information about a specific node in GraphX
    GraphX Pregel API
    Run 1 iteration, with every node passing its identity to all its neighbors
Need to copy graph to every machine (goes wrong with big graphs)
    We didn’t find a good solution for this problem
    How can we avoid copying the whole graph to every machine?

Normally, betweenness is hard to calculate. distributed betweenness is very hard. It is very hard over 1M nodes to calculate.
But using distributed approach, it is possible.

Borgatti and Everett (2006) define the k-betweenness of a vertex as the sum of dependencies of pairs at most k part.
To calculate a node's Betweenness contribution to other nodes, we only need to consider nodes at distance <_ k

https://www.slideshare.net/DanielMarcous/distributed-kbetweenness-spark
https://spark-packages.org/package/dmarcous/spark-betweenness
https://github.com/dmarcous/spark-betweenness


Closeness Centrality:
average distance of every single of node to everyone else.
In a connected graph, the normalized closeness centrality (or closeness) of a node is the average length of the shortest path between the node and all other nodes in the graph. Thus the more central a node is, the closer it is to all other nodes.


Harmonic Centrality:
Harmonic centrality of a node x is the sum of the reciprocal of the shortest path distances from all other nodes to x.
It uses HyperLogLog algorithm.

Harmonic centrality : https://en.wikipedia.org/wiki/Centrality#Closeness_centrality
In a (not necessarily connected) graph, the harmonic centrality reverses the sum and reciprocal operations in the definition of closeness centrality:
Harmonic centrality can be normalized by dividing by {\displaystyle N-1}N-1, where {\displaystyle N}N is the number of nodes in the graph.


Harmonic centrality was proposed by Marchiori and Latora (2000)[18] and then independently by Dekker (2005), using the name "valued centrality,"[19] and by Rochat (2009).[20]

https://spark-packages.org/package/webgeist/spark-centrality
https://github.com/pvgladkov/spark-centrality
https://events.yandex.ru/lib/talks/1287/
http://infoscience.epfl.ch/record/200525/files/%5BEN%5DASNA09.pdf
https://en.wikipedia.org/wiki/Centrality